{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 9 Week 2\n",
    "## Overview\n",
    "In the next three notebooks, we'll be developing an NLP system to extract mentions of pneumonia from clinical text. To do this, we'll use an open-source package called [pyConText](https://github.com/chapmanbe/pyConTextNLP/tree/master/pyConTextNLP) which leverages regular expressions and NetworkX graphs to identify concepts and their contexts in text.\n",
    "\n",
    "First, we'll get familiar with the dataset and task by looking at a **gold standard** of human-annotated documents which we'll compare to our NLP system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT NOTE\n",
    "pyConText **does not support** Python version 2.7. A number of the computers at UVU seem to use Python 2.7 as the default. You should instead use at least Python 3.5 or higher (I use 3.7). \n",
    "\n",
    "To check your Python version, either run this cell below or copy and paste this command into Anaconda Prompt. If you are running Python 2, follow the instructions on Canvas to create a conda environment with Python 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "import sklearn.metrics\n",
    "\n",
    "# packages for interaction\n",
    "from IPython.html.widgets import interact, interactive, fixed\n",
    "from IPython.display import display, HTML, Image\n",
    "import ipywidgets\n",
    "\n",
    "# and also our utilities for this class\n",
    "from nlp_pneumonia_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Annotation\n",
    "When designing an NLP system, we need examples to compare our system with a human's judgment. This allows us to see examples where our system makes mistakes and to measure metrics such as **accuracy**, **precision**, and **recall**.\n",
    "\n",
    "One way to gather this information is by **annotating** clinical text. In an annotation study, human experts will read through a small number of clinical documents and manually extract the information of interest. These annotations then become part of a **reference standard** which we use to evaluate our system.\n",
    "\n",
    "# Pneumonia Dataset\n",
    "Today, we'll be working with an annotated dataset of MIMIC-II radiology reports. Our training set will consist of 100 documents which were reviewed and marked for:\n",
    "- **Mention-level evidence**: Individual phrases or sentences which the annotators considered evidence of pneumonia\n",
    "- **Document-level classification**: Whether or not the document indicates the patient has pneumonia\n",
    "\n",
    "We'll start by looking through the annotated dataset to get a sense of what our task is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data from our training oflder\n",
    "annotated_doc_map = read_doc_annotations('pneumonia_data/training_v2')\n",
    "annotated_docs = list(annotated_doc_map.values())\n",
    "\n",
    "print('Total Annotated Documents : {0}'.format(len(annotated_docs)))\n",
    "\n",
    "total_positives = 0\n",
    "for anno_doc in annotated_docs:\n",
    "    if anno_doc.positive_label:\n",
    "        total_positives += 1\n",
    "    \n",
    "print('Total Positive Pneumonia Documents : {0}'.format(total_positives))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = annotated_doc_map_to_df(annotated_doc_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['annotation_level'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what this dataset looks like. We can scroll through one document at a time and view a marked-up version of our document, plus look at the structured annotations.\n",
    "\n",
    "Take a few minutes to scroll through the documents. Positive mention-level annotations of pneumonia will be highlighted red within the text.\n",
    "\n",
    "**Discussion**\n",
    "- What phrases/words seem to mean \"pneumonia\"?\n",
    "- Are there any documents which have the word \"pneumonia\" but aren't highlighted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function let's us iterate through all documents and view the markup\n",
    "def view_annotation_markup(anno_docs):\n",
    "    @interact(i=ipywidgets.IntSlider(min=0, max=len(anno_docs)-1))\n",
    "    def _view_markup(i):\n",
    "        report_html = pneumonia_annotation_html_markup(anno_docs[i])\n",
    "        report_html = report_html.replace('\\n', '<br>')\n",
    "        display(HTML(report_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 2\n",
    "sub_df = df[df['document_idx'] == index]\n",
    "sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_annotation_markup(annotated_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

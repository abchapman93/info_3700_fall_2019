{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import getpass\n",
    "\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter password for MIMIC2 database········\n",
      "Importing pymysql failed. Using cached data instead\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import pymysql\n",
    "    conn = pymysql.connect(host=\"35.233.174.193\",port=3306,\n",
    "                           user=\"jovyan\",passwd=getpass.getpass(\"Enter password for MIMIC2 database\"),\n",
    "                           db='mimic2')\n",
    "    CACHED = False\n",
    "except Exception as e:\n",
    "    print(\"Importing pymysql failed. Using cached data instead\")\n",
    "    CACHED = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clinical Notes\n",
    "One of the most information-rich sources of clinical data are free-text clinical notes. These are written narratives which discuss many topics of a patient's care, such as diagnoses, history, treatment, complications, etc... Free-text notes offer very detailed accounts of a patient's clinical course, making them very useful for many purposes. However, free text is not inherently meaningful. Unlike ICD-9 or LOINC codes, clinical language does not offer standardized representations of clinical concepts, and unlike vitals and labs, the information found in text cannot be easily quantified and represented in a computable way.\n",
    "\n",
    "The field of **natural language processing** offers methods for dealing with text and extracting **structured information** from an **unstructured data source**. Later in the semester we'll have a module specifically devoted to NLP. Today we'll look at a couple of text processing methods which give us some insight into the information contained in text.\n",
    "\n",
    "## Note types\n",
    "There are many different types of notes in the clinical domain. Each note contains different information, often specific to clinical specialties like cardiology or surgery. One such specialty is radiology. MIMIC-II contains a large number of **radiology reports** which contain a radiologist's interpretation of an image. For example, if a physician suspects a patient has pneumonia, they might order a CT scan. The radiologist will view the image from the scan and determine whether or not has a patient has pneumonia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying notes\n",
    "Clinical notes are contained in the table `noteevents`. We'll specifically query radiology notes. Additionally, since there are a large number of notes we'll limit our queries to only look at 1000 notes, although later we'll use a language model which was trained using all of the radiology notes in the database.\n",
    "\n",
    "### TODO\n",
    "Finish the query below to query the `noteevents` table and limit the results to notes where the category is \"RADIOLOGY_REPORT\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "select text from noteevents\n",
    "where category = 'RADIOLOGY_REPORT'\n",
    "limit 1000;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading saved files\n"
     ]
    }
   ],
   "source": [
    "# If we were able to import pymysql, we'll query MIMIC-II\n",
    "if not CACHED:\n",
    "    print(\"Querying MIMIC\")\n",
    "    df = pd.read_sql(query, conn)\n",
    "# Otherwise, we'll use a smaller sample of documents in the repo\n",
    "else:\n",
    "    print(\"Reading saved files\")\n",
    "    df = helpers.read_pneumonia_documents('./pneumonia_data/training_v2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>\\n\\n\\n     DATE: [**2717-5-23**] 12:06 PM\\n   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>\\n\\n\\n     DATE: [**3368-3-20**] 11:46 AM\\n   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>\\n\\n\\n     DATE: [**3106-8-10**] 11:46 AM\\n   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>\\n\\n\\n     DATE: [**3469-9-13**] 6:54 PM\\n    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>\\n\\n\\n     DATE: [**3346-10-31**] 4:35 PM\\n   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  \\n\\n\\n     DATE: [**2717-5-23**] 12:06 PM\\n   ...\n",
       "1  \\n\\n\\n     DATE: [**3368-3-20**] 11:46 AM\\n   ...\n",
       "2  \\n\\n\\n     DATE: [**3106-8-10**] 11:46 AM\\n   ...\n",
       "3  \\n\\n\\n     DATE: [**3469-9-13**] 6:54 PM\\n    ...\n",
       "4  \\n\\n\\n     DATE: [**3346-10-31**] 4:35 PM\\n   ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at what one of these notes looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "     DATE: [**2717-5-23**] 12:06 PM\n",
      "     CHEST (PORTABLE AP); -76 BY SAME PHYSICIAN                      [**Name Initial (PRE) 58**] # [**Clip Number (Radiology) 4359**]\n",
      "     Reason: s/p brochoscopy                                             \n",
      "     Admitting Diagnosis: S/P FALL\n",
      "     ______________________________________________________________________________\n",
      "     UNDERLYING MEDICAL CONDITION:\n",
      "        83 year old F w/ MS s/p fall. now intubated w/ desats.    leukocytosis.       \n",
      "                                               \n",
      "     REASON FOR THIS EXAMINATION:\n",
      "      s/p brochoscopy                                                                 \n",
      "     ______________________________________________________________________________\n",
      "                                     FINAL REPORT\n",
      "     INDICATION:  MS status post fall, now intubated with desaturations and\n",
      "     leukocytosis, status post bronchoscopy.\n",
      "     \n",
      "     COMPARISON:  [**2717-5-23**] at 8:51 a.m.\n",
      "     \n",
      "     TECHNIQUE:  Single AP portable supine chest.\n",
      "     \n",
      "     FINDINGS:  Since the examination of several hours earlier, the endotracheal\n",
      "     tube, nasogastric tube, and right subclavian venous access catheter appear in\n",
      "     unchanged position.  Heart size and mediastinal contours are unchanged.  There\n",
      "     is continued improvement in mild congestive heart failure.  Right pleural\n",
      "     effusion unchanged in size.  Note is again made of biapical pleural\n",
      "     thickening.  No pneumothorax.\n",
      "     \n",
      "     IMPRESSION:  Improving congestive heart failure and stable right pleural\n",
      "     effusion.  Lines and tubes in unchanged position.\n",
      "                                                                           \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df['text'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword search\n",
    "One of the most basic things we can do with free text is to do a **keyword search**. Similar to a Google search, we want to find a set of documents which contain a specific phrase. In SQL, we can do this by using the `like` statement, which allows you to use wildcards. For example, the SQL clause `where text like %adve%` would return documents containing the words \"adventure\", \"adventures\", \"advertisement\", \"advertise\", etc...\n",
    "\n",
    "### TODO\n",
    "Limit the query below to only return documents where the text contains the word \"pneumonia\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "select subject_id, text from noteevents\n",
    "where category = 'RADIOLOGY_REPORT'\n",
    "    and text ___ '___'\n",
    "limit 100\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not CACHED:\n",
    "    df = pd.read_sql(query, conn)\n",
    "else: # Use a Regex filter in pandas\n",
    "    df = df[df['text'].str.contains('pneumonia')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read through a few examples of the document. Where is pneumonia discussed? Do the patients actually have pneumonia? If not, why is it being mentioned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text processing\n",
    "Before doing any sort of computation with these texts, there are a number of steps to take to make the data easier to work with. Clinical text is **very** messy: it is very inconsistent, confusing, and often ugly. **Preprocessing** is where we clean up the text a little bit. Some possible steps for preprocessing include:\n",
    "- Converting the text to lower case\n",
    "- Replacing **\"stop words\"**: words or phrases which occur so often that they don't contain any useful information (\"and\", \"or\", \"the\", etc...)\n",
    "- Merging phrases of 2 or more words\n",
    "- Splitting texts into **\"tokens\"** (ie., individual words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this command to download some data needed for text processing:\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import text_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original texts\n",
    "texts = list(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessed texts\n",
    "preprocessed_texts = [text_processing.preprocess(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenized texts\n",
    "tokenized_texts = [text_processing.tokenize(text, rm_stopwords=True) for text in preprocessed_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preprocessed_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple word counts\n",
    "Now that we've preprocessed our text, we can do some very basic operations on it. Let's count how many times each word occurs and see what the most frequent words are. This will be useful for getting a high-level sense of what information is in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "counter = defaultdict(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tokens in tokenized_texts:\n",
    "    for token in tokens:\n",
    "        counter[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srtd_word_counts = sorted(counter.items(), key=lambda x:x[1], reverse=True)\n",
    "srtd_word_counts[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One nice way to visualize this is with a wordcloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud()\n",
    "wordcloud.generate_from_frequencies(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "Now, let's get into something a little more sophisticated. In the steps above, all we did was iterate through the documents and count how many times each word occurred. This gives us a high-level sense of what words are in this vocabulary, but it doesn't tell us anything about the **meaning** or **semantics** of these words.\n",
    "\n",
    "In this next exercise we'll look at how we can use **machine learning** to generate some **semantic meaning** from the text. A method called **word embeddings** transforms words, which by default have no computational or semantic meaning, into vectors which contain some representation of what the words meaning. We won't go into the details here, but a quick summary is that we look at the **context** of word - the words nearby a target word - to estimate what it means. Words which occur in similar contexts probably mean similar things. \n",
    "\n",
    "For example, consider these 3 sentences:\n",
    "- \"I have a **dog** for a pet\"\n",
    "- \"I have a **cat** for a pet\"\n",
    "- \"I have a **fish** for a pet\"\n",
    "\n",
    "Since the context around \"dog\", \"cat\", and \"fish\" is very similar, they are probably similar semantically. We translate words into vectors, and words which have similar meanings have vectors. These vectors are called **word embeddings** and we can use them to measure the similarity between different words. If you're interested in learning more, here's a tutorial to get you started with word embeddings: https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa\n",
    "\n",
    "One algorithm for generating word embeddings is called `word2vec`. I pretrained a word2vec model on all of the MIMIC-II radiology reports and saved it as a pickle file. Let's read this model in and do some experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the model corresponding to your Python version\n",
    "saved_model_filepath = \"./trained_word2vec_py{}.pkl\".format(sys.version_info[0])\n",
    "print(saved_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip the trained model\n",
    "import zipfile\n",
    "with zipfile.ZipFile(saved_model_filepath+'.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(saved_model_filepath, 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's pick a target word which will occur in our vocabulary: **\"abdomen\"**. Let's first see what the embedding for \"abdomen\" looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv['abdomen']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take two other words: \"thorax\" and \"radiograph\". Which do you think is more similar to abdomen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Similarity between 'abdomen' and 'thorax':\", model.wv.similarity('abdomen', 'thorax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Similarity between 'abdomen' and 'radiograph':\", model.wv.similarity('abdomen', 'radiograph'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find what terms our model thinks are most similar to \"abdomen\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(['abdomen'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty cool! This is an example of how machine learning can be used to derive insights from raw data. It also shows how we can transform raw text, which lacks any defined structure or semantics, and generate some meaning out of it. \n",
    "\n",
    "### TODO\n",
    "**Vocabulary expansion**\n",
    "Let's look at two concept classes: *medications* and *diagnoses*. Let's say that we know 1-2 words for each class, but we want to come up with a more complete list. Rather than asking a physician to list all of the medications and diseases that they know, can we use word embeddings to find similar words?\n",
    "\n",
    "Below I've given seed words for each class. Go through the suggestions from word2vec and see how many of each class you can identify using similarity metrics with word2vec. Note that you can give the model a list of words and it will find words which are similar to all of the words in that list, which can help guide your model to find the most similar terms. \n",
    "\n",
    "As you're doing this, consider what kinds of words are being returned. Are they similar to the seed words you're starting with? How are they related? Try doing some other classes as well.\n",
    "\n",
    "You can google abbreviations or words you don't know to see what they mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnoses\n",
    "model.wv.most_similar(['pneumonia'], topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medications\n",
    "model.wv.most_similar(['heparin', 'coumadin'], topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
